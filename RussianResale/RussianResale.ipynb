{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7889624,"sourceType":"datasetVersion","datasetId":4631911},{"sourceId":7889636,"sourceType":"datasetVersion","datasetId":4631920}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"* Download dependecies section","metadata":{"id":"mCvHZeComhI8"}},{"cell_type":"code","source":"!pip install --upgrade --quiet pip\n!pip install --quiet transformers datasets accelerate deepspeed pymorphy2 peft trl==0.7.4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RchDAcKwmZu8","outputId":"54d16661-a26e-499b-e62e-4903d5d6ffad","execution":{"iopub.status.busy":"2024-03-20T07:06:43.802580Z","iopub.execute_input":"2024-03-20T07:06:43.802939Z","iopub.status.idle":"2024-03-20T07:07:41.550420Z","shell.execute_reply.started":"2024-03-20T07:06:43.802909Z","shell.execute_reply":"2024-03-20T07:07:41.549266Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"* Import section","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport re\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom collections import Counter\nfrom matplotlib import pyplot as plt\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding, XLMRobertaModel, XLMRobertaConfig\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, recall_score\nfrom nltk.corpus import stopwords\nfrom pymorphy2 import MorphAnalyzer\nfrom peft import LoraConfig\nfrom trl import SFTTrainer\nfrom gensim.models import KeyedVectors","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:07:41.552312Z","iopub.execute_input":"2024-03-20T07:07:41.552601Z","iopub.status.idle":"2024-03-20T07:08:23.386141Z","shell.execute_reply.started":"2024-03-20T07:07:41.552572Z","shell.execute_reply":"2024-03-20T07:08:23.385380Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-03-20 07:07:48.441875: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-20 07:07:48.442007: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-20 07:07:48.566062: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"[2024-03-20 07:07:58,176] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"* Importing data","metadata":{}},{"cell_type":"code","source":"data = pd.read_excel('/kaggle/input/dataset5/Feedback_data.xlsx', index_col=0)\ndata","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"Y8PnFUonmuM7","outputId":"5cde3a59-ca08-4261-dce1-6e551144607e","execution":{"iopub.status.busy":"2024-03-20T07:08:23.387134Z","iopub.execute_input":"2024-03-20T07:08:23.387410Z","iopub.status.idle":"2024-03-20T07:08:24.032640Z","shell.execute_reply.started":"2024-03-20T07:08:23.387386Z","shell.execute_reply":"2024-03-20T07:08:24.031736Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"         id                                               text  \\\n0        45   негатив к вам и горячей линии которые нафиг н...   \n1        97  перестаньте впаривать клиентам дополнительные ...   \n2       105   продавец предложил наклеить защитную плёнку з...   \n3       127  продавец режил продать нам товар почему то по ...   \n4       130  навязанные услуги ( без моего согласия) на сум...   \n...     ...                                                ...   \n2832  17570  почему отключили электронную очередь через пол...   \n2833  17579  очередь на кассе. продавцы не торопятся с зака...   \n2834  17586  профессионализма менеджеру бы, да и внимания к...   \n2835  17591  сотрудник на выдаче заказа потребовал документ...   \n2836  17594                           клиентоориентированность   \n\n                                        class  \n0                             Консультация КЦ  \n1     Компетентность продавцов/ консультантов  \n2     Компетентность продавцов/ консультантов  \n3     Компетентность продавцов/ консультантов  \n4     Компетентность продавцов/ консультантов  \n...                                       ...  \n2832                      Электронная очередь  \n2833                   Время ожидания у кассы  \n2834  Обслуживание продавцами/ консультантами  \n2835                      Электронная очередь  \n2836  Компетентность продавцов/ консультантов  \n\n[2837 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>45</td>\n      <td>негатив к вам и горячей линии которые нафиг н...</td>\n      <td>Консультация КЦ</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>97</td>\n      <td>перестаньте впаривать клиентам дополнительные ...</td>\n      <td>Компетентность продавцов/ консультантов</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>105</td>\n      <td>продавец предложил наклеить защитную плёнку з...</td>\n      <td>Компетентность продавцов/ консультантов</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>127</td>\n      <td>продавец режил продать нам товар почему то по ...</td>\n      <td>Компетентность продавцов/ консультантов</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>130</td>\n      <td>навязанные услуги ( без моего согласия) на сум...</td>\n      <td>Компетентность продавцов/ консультантов</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2832</th>\n      <td>17570</td>\n      <td>почему отключили электронную очередь через пол...</td>\n      <td>Электронная очередь</td>\n    </tr>\n    <tr>\n      <th>2833</th>\n      <td>17579</td>\n      <td>очередь на кассе. продавцы не торопятся с зака...</td>\n      <td>Время ожидания у кассы</td>\n    </tr>\n    <tr>\n      <th>2834</th>\n      <td>17586</td>\n      <td>профессионализма менеджеру бы, да и внимания к...</td>\n      <td>Обслуживание продавцами/ консультантами</td>\n    </tr>\n    <tr>\n      <th>2835</th>\n      <td>17591</td>\n      <td>сотрудник на выдаче заказа потребовал документ...</td>\n      <td>Электронная очередь</td>\n    </tr>\n    <tr>\n      <th>2836</th>\n      <td>17594</td>\n      <td>клиентоориентированность</td>\n      <td>Компетентность продавцов/ консультантов</td>\n    </tr>\n  </tbody>\n</table>\n<p>2837 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Converting to numpy\nX = data['text'].to_numpy()\nY = data['class'].to_numpy()","metadata":{"id":"1wF79W9QsEsR","execution":{"iopub.status.busy":"2024-03-20T07:08:24.035076Z","iopub.execute_input":"2024-03-20T07:08:24.036252Z","iopub.status.idle":"2024-03-20T07:08:24.042535Z","shell.execute_reply.started":"2024-03-20T07:08:24.036224Z","shell.execute_reply":"2024-03-20T07:08:24.041678Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"* Exploration analysis","metadata":{"id":"45OYxwtytg-d"}},{"cell_type":"code","source":"# We can see that there is a slight class imbalance in data, I think that this is not a crucial\n# May be we need to adjust an addition weight while training\n\ndata.groupby('class').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\nplt.gca().spines[['top', 'right',]].set_visible(False)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":430},"id":"fv1Tjs8Cpseq","outputId":"5c8723e9-ee6f-4696-9fac-2f8b2ac8537b","execution":{"iopub.status.busy":"2024-03-20T07:08:24.043865Z","iopub.execute_input":"2024-03-20T07:08:24.044197Z","iopub.status.idle":"2024-03-20T07:08:24.403543Z","shell.execute_reply.started":"2024-03-20T07:08:24.044165Z","shell.execute_reply":"2024-03-20T07:08:24.402611Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"# Assert that all id's are unique and there are no missing values\n\nassert data['id'].value_counts().sum() == data.shape[0]\ndata.isnull().sum()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V6quJynktir2","outputId":"338fd5d1-7fb7-4ba9-bf9c-96e9f076e7f5","execution":{"iopub.status.busy":"2024-03-20T07:08:24.404819Z","iopub.execute_input":"2024-03-20T07:08:24.405087Z","iopub.status.idle":"2024-03-20T07:08:24.417919Z","shell.execute_reply.started":"2024-03-20T07:08:24.405063Z","shell.execute_reply":"2024-03-20T07:08:24.416988Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"id       0\ntext     0\nclass    0\ndtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"* Data Preparation","metadata":{"id":"zIHlhypEv6eg"}},{"cell_type":"code","source":"# Let's prepare data for tokenization\n\npatterns = \"[A-Za-z0-9!#$%&'()*+,./:;<=>?@[\\]^_`{|}~—\\\"\\-]+\"\nmorph = MorphAnalyzer()\n\nlabel2ind = {label: i for i, label in enumerate(data['class'].unique())}\nind2label = {v: k for k, v in label2ind.items()}\n\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, x, y, label2ind, tokenizer=None):\n        super(Dataset).__init__()\n        self.texts = x\n        self.labels = y\n        self.label2ind = label2ind\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n        output = self.prepare_example(self.texts[index])\n        output['label'] = label2ind[self.labels[index]]\n        return output\n\n    def prepare_example(self, text):\n        text = re.sub(patterns, ' ', text)\n        tokens = []\n        for token in text.split():\n            token = token.strip()\n            token = morph.normal_forms(token)[0]\n            tokens.append(token)\n        return self.tokenizer(' '.join(tokens))","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:08:24.419227Z","iopub.execute_input":"2024-03-20T07:08:24.419599Z","iopub.status.idle":"2024-03-20T07:08:24.638404Z","shell.execute_reply.started":"2024-03-20T07:08:24.419561Z","shell.execute_reply":"2024-03-20T07:08:24.637666Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"* Baseline: we will use roberta trained on russian texts","metadata":{}},{"cell_type":"code","source":"model_name = 'blinoff/roberta-base-russian-v0'\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=8)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nfor (name, module) in model.roberta.named_modules():\n    for parameter in module.parameters():\n        parameter.requires_grad = False\n\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:08:24.639520Z","iopub.execute_input":"2024-03-20T07:08:24.639830Z","iopub.status.idle":"2024-03-20T07:08:29.665459Z","shell.execute_reply.started":"2024-03-20T07:08:24.639804Z","shell.execute_reply":"2024-03-20T07:08:29.664528Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7088b2a66aa94ee796e8f5897cb00d9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/500M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0f369f9f71d407e88ecfed45384b853"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at blinoff/roberta-base-russian-v0 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/16.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1deddc7b4db3468a9783d8b528c53930"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.68M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fd932368dae43548b682372323e176e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.34M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71d4f816e5604b3580c4e2eef741d554"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c289ebbd3a46416392a9bfcc8d6aaf8e"}},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50021, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=8, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Splitting data into train and val sets\nx_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.1, random_state=52, stratify=Y)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:08:29.666686Z","iopub.execute_input":"2024-03-20T07:08:29.667014Z","iopub.status.idle":"2024-03-20T07:08:29.679139Z","shell.execute_reply.started":"2024-03-20T07:08:29.666988Z","shell.execute_reply":"2024-03-20T07:08:29.678268Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Creating Datasets\ntrainDataset = Dataset(x_train, y_train, label2ind, tokenizer)\nvalDataset = Dataset(x_val, y_val, label2ind, tokenizer)","metadata":{"id":"uQCnrBzsv97B","execution":{"iopub.status.busy":"2024-03-20T07:08:29.683635Z","iopub.execute_input":"2024-03-20T07:08:29.683931Z","iopub.status.idle":"2024-03-20T07:08:29.712075Z","shell.execute_reply.started":"2024-03-20T07:08:29.683906Z","shell.execute_reply":"2024-03-20T07:08:29.711279Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"* Define Baseline Trainer","metadata":{"id":"4a9wWfyfpj9r"}},{"cell_type":"code","source":"def compute_metrics(p):\n    preds = np.argmax(p.predictions, axis=1)\n    labels = p.label_ids\n\n    # Calculate recall for each class\n    recalls = recall_score(labels, preds, average=None, labels=[0, 1, 2, 3, 4, 5, 6, 7]) # Adjust labels based on your classes\n\n    # Create a dictionary to store recall for each class\n    results = {}\n    for i, recall in enumerate(recalls):\n        results[f'class_{i}_recall'] = recall\n    \n    f1 = f1_score(labels, preds, average='macro')\n    results['F1'] = f1\n\n    return results\n\n\n\n# Setting congifurations for LORA PEFT\npeft_config = LoraConfig(\n    lora_alpha=8,\n    lora_dropout=0.05,\n    r=8,\n    bias=\"none\",\n    target_modules = [\"query\", \"key\", \"value\", \"dense\"],\n    task_type=\"SEQ_CLS\",\n)\n\ntraining_arguments = TrainingArguments(\n    output_dir=\"logs\",\n    num_train_epochs=50,\n    per_device_train_batch_size=16,\n    save_strategy='no',\n    evaluation_strategy=\"epoch\"\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    args=training_arguments,\n    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n    train_dataset=trainDataset,\n    eval_dataset=valDataset,\n    compute_metrics=compute_metrics,\n    peft_config=peft_config,\n    dataset_text_field='text'\n)","metadata":{"id":"odgacVFhoxEe","execution":{"iopub.status.busy":"2024-03-20T07:08:29.713181Z","iopub.execute_input":"2024-03-20T07:08:29.713466Z","iopub.status.idle":"2024-03-20T07:08:30.442306Z","shell.execute_reply.started":"2024-03-20T07:08:29.713442Z","shell.execute_reply":"2024-03-20T07:08:30.441533Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:194: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 512\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# We can see that the best F1 score is 0.77. Let's upgrade it further\ntrainer.train()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"z4NUrq9n9Ti8","outputId":"0e35847e-2542-4183-e867-60ddd014a90c","execution":{"iopub.status.busy":"2024-03-18T16:54:01.286939Z","iopub.execute_input":"2024-03-18T16:54:01.287696Z","iopub.status.idle":"2024-03-18T17:50:15.234885Z","shell.execute_reply.started":"2024-03-18T16:54:01.287666Z","shell.execute_reply":"2024-03-18T17:50:15.233792Z"},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2000/2000 56:10, Epoch 50/50]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Class 0 Recall</th>\n      <th>Class 1 Recall</th>\n      <th>Class 2 Recall</th>\n      <th>Class 3 Recall</th>\n      <th>Class 4 Recall</th>\n      <th>Class 5 Recall</th>\n      <th>Class 6 Recall</th>\n      <th>Class 7 Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.877343</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.020000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.061722</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>1.769306</td>\n      <td>0.000000</td>\n      <td>0.963855</td>\n      <td>0.000000</td>\n      <td>0.172414</td>\n      <td>0.080000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.258065</td>\n      <td>0.152120</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>1.608026</td>\n      <td>0.000000</td>\n      <td>0.903614</td>\n      <td>0.192308</td>\n      <td>0.620690</td>\n      <td>0.180000</td>\n      <td>0.000000</td>\n      <td>0.037037</td>\n      <td>0.451613</td>\n      <td>0.281105</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>1.467999</td>\n      <td>0.000000</td>\n      <td>0.939759</td>\n      <td>0.192308</td>\n      <td>0.655172</td>\n      <td>0.140000</td>\n      <td>0.047619</td>\n      <td>0.185185</td>\n      <td>0.612903</td>\n      <td>0.332800</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>1.344313</td>\n      <td>0.000000</td>\n      <td>0.843373</td>\n      <td>0.653846</td>\n      <td>0.620690</td>\n      <td>0.300000</td>\n      <td>0.047619</td>\n      <td>0.148148</td>\n      <td>0.580645</td>\n      <td>0.378518</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>No log</td>\n      <td>1.223782</td>\n      <td>0.000000</td>\n      <td>0.927711</td>\n      <td>0.576923</td>\n      <td>0.689655</td>\n      <td>0.380000</td>\n      <td>0.095238</td>\n      <td>0.111111</td>\n      <td>0.677419</td>\n      <td>0.422478</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>No log</td>\n      <td>1.113350</td>\n      <td>0.000000</td>\n      <td>0.915663</td>\n      <td>0.846154</td>\n      <td>0.758621</td>\n      <td>0.500000</td>\n      <td>0.142857</td>\n      <td>0.074074</td>\n      <td>0.774194</td>\n      <td>0.485236</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>No log</td>\n      <td>1.032637</td>\n      <td>0.000000</td>\n      <td>0.759036</td>\n      <td>0.846154</td>\n      <td>0.724138</td>\n      <td>0.560000</td>\n      <td>0.476190</td>\n      <td>0.259259</td>\n      <td>0.806452</td>\n      <td>0.549660</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>No log</td>\n      <td>0.995207</td>\n      <td>0.000000</td>\n      <td>0.867470</td>\n      <td>0.846154</td>\n      <td>0.724138</td>\n      <td>0.460000</td>\n      <td>0.476190</td>\n      <td>0.296296</td>\n      <td>0.774194</td>\n      <td>0.558984</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>No log</td>\n      <td>0.962788</td>\n      <td>0.000000</td>\n      <td>0.855422</td>\n      <td>0.884615</td>\n      <td>0.724138</td>\n      <td>0.540000</td>\n      <td>0.523810</td>\n      <td>0.296296</td>\n      <td>0.774194</td>\n      <td>0.571661</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>No log</td>\n      <td>0.932580</td>\n      <td>0.117647</td>\n      <td>0.783133</td>\n      <td>0.884615</td>\n      <td>0.689655</td>\n      <td>0.600000</td>\n      <td>0.571429</td>\n      <td>0.333333</td>\n      <td>0.838710</td>\n      <td>0.608440</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>No log</td>\n      <td>0.912577</td>\n      <td>0.058824</td>\n      <td>0.843373</td>\n      <td>0.846154</td>\n      <td>0.689655</td>\n      <td>0.600000</td>\n      <td>0.523810</td>\n      <td>0.296296</td>\n      <td>0.838710</td>\n      <td>0.590543</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>1.276800</td>\n      <td>0.906107</td>\n      <td>0.176471</td>\n      <td>0.879518</td>\n      <td>0.884615</td>\n      <td>0.758621</td>\n      <td>0.620000</td>\n      <td>0.571429</td>\n      <td>0.185185</td>\n      <td>0.870968</td>\n      <td>0.626003</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>1.276800</td>\n      <td>0.859037</td>\n      <td>0.235294</td>\n      <td>0.855422</td>\n      <td>0.884615</td>\n      <td>0.655172</td>\n      <td>0.640000</td>\n      <td>0.571429</td>\n      <td>0.296296</td>\n      <td>0.838710</td>\n      <td>0.638135</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>1.276800</td>\n      <td>0.845331</td>\n      <td>0.294118</td>\n      <td>0.819277</td>\n      <td>0.884615</td>\n      <td>0.724138</td>\n      <td>0.660000</td>\n      <td>0.619048</td>\n      <td>0.333333</td>\n      <td>0.838710</td>\n      <td>0.659158</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>1.276800</td>\n      <td>0.823191</td>\n      <td>0.294118</td>\n      <td>0.819277</td>\n      <td>0.884615</td>\n      <td>0.724138</td>\n      <td>0.700000</td>\n      <td>0.619048</td>\n      <td>0.296296</td>\n      <td>0.903226</td>\n      <td>0.668860</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>1.276800</td>\n      <td>0.808776</td>\n      <td>0.294118</td>\n      <td>0.843373</td>\n      <td>0.884615</td>\n      <td>0.655172</td>\n      <td>0.660000</td>\n      <td>0.619048</td>\n      <td>0.259259</td>\n      <td>0.870968</td>\n      <td>0.651455</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>1.276800</td>\n      <td>0.809205</td>\n      <td>0.588235</td>\n      <td>0.783133</td>\n      <td>0.884615</td>\n      <td>0.724138</td>\n      <td>0.680000</td>\n      <td>0.666667</td>\n      <td>0.333333</td>\n      <td>0.870968</td>\n      <td>0.695965</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>1.276800</td>\n      <td>0.764398</td>\n      <td>0.411765</td>\n      <td>0.795181</td>\n      <td>0.884615</td>\n      <td>0.758621</td>\n      <td>0.720000</td>\n      <td>0.666667</td>\n      <td>0.444444</td>\n      <td>0.870968</td>\n      <td>0.708321</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.276800</td>\n      <td>0.756957</td>\n      <td>0.411765</td>\n      <td>0.819277</td>\n      <td>0.884615</td>\n      <td>0.655172</td>\n      <td>0.740000</td>\n      <td>0.619048</td>\n      <td>0.333333</td>\n      <td>0.870968</td>\n      <td>0.687702</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>1.276800</td>\n      <td>0.756138</td>\n      <td>0.588235</td>\n      <td>0.843373</td>\n      <td>0.884615</td>\n      <td>0.793103</td>\n      <td>0.660000</td>\n      <td>0.666667</td>\n      <td>0.407407</td>\n      <td>0.870968</td>\n      <td>0.725457</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>1.276800</td>\n      <td>0.739063</td>\n      <td>0.470588</td>\n      <td>0.807229</td>\n      <td>0.884615</td>\n      <td>0.655172</td>\n      <td>0.760000</td>\n      <td>0.619048</td>\n      <td>0.370370</td>\n      <td>0.838710</td>\n      <td>0.693848</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>1.276800</td>\n      <td>0.741838</td>\n      <td>0.588235</td>\n      <td>0.855422</td>\n      <td>0.884615</td>\n      <td>0.827586</td>\n      <td>0.720000</td>\n      <td>0.666667</td>\n      <td>0.370370</td>\n      <td>0.903226</td>\n      <td>0.740086</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>1.276800</td>\n      <td>0.729919</td>\n      <td>0.588235</td>\n      <td>0.855422</td>\n      <td>0.923077</td>\n      <td>0.827586</td>\n      <td>0.720000</td>\n      <td>0.666667</td>\n      <td>0.333333</td>\n      <td>0.870968</td>\n      <td>0.732616</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.673300</td>\n      <td>0.737658</td>\n      <td>0.705882</td>\n      <td>0.831325</td>\n      <td>0.923077</td>\n      <td>0.793103</td>\n      <td>0.740000</td>\n      <td>0.666667</td>\n      <td>0.333333</td>\n      <td>0.903226</td>\n      <td>0.737374</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.673300</td>\n      <td>0.720626</td>\n      <td>0.588235</td>\n      <td>0.843373</td>\n      <td>0.923077</td>\n      <td>0.827586</td>\n      <td>0.740000</td>\n      <td>0.666667</td>\n      <td>0.518519</td>\n      <td>0.870968</td>\n      <td>0.756549</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.673300</td>\n      <td>0.707215</td>\n      <td>0.647059</td>\n      <td>0.855422</td>\n      <td>0.923077</td>\n      <td>0.793103</td>\n      <td>0.760000</td>\n      <td>0.714286</td>\n      <td>0.370370</td>\n      <td>0.870968</td>\n      <td>0.748314</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.673300</td>\n      <td>0.708481</td>\n      <td>0.588235</td>\n      <td>0.867470</td>\n      <td>0.923077</td>\n      <td>0.724138</td>\n      <td>0.760000</td>\n      <td>0.714286</td>\n      <td>0.407407</td>\n      <td>0.870968</td>\n      <td>0.742659</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.673300</td>\n      <td>0.725908</td>\n      <td>0.588235</td>\n      <td>0.867470</td>\n      <td>0.923077</td>\n      <td>0.793103</td>\n      <td>0.720000</td>\n      <td>0.666667</td>\n      <td>0.444444</td>\n      <td>0.870968</td>\n      <td>0.748880</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.673300</td>\n      <td>0.705067</td>\n      <td>0.705882</td>\n      <td>0.795181</td>\n      <td>0.923077</td>\n      <td>0.793103</td>\n      <td>0.760000</td>\n      <td>0.714286</td>\n      <td>0.407407</td>\n      <td>0.903226</td>\n      <td>0.748844</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.673300</td>\n      <td>0.711545</td>\n      <td>0.705882</td>\n      <td>0.855422</td>\n      <td>0.923077</td>\n      <td>0.793103</td>\n      <td>0.720000</td>\n      <td>0.666667</td>\n      <td>0.444444</td>\n      <td>0.903226</td>\n      <td>0.759032</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.673300</td>\n      <td>0.691491</td>\n      <td>0.705882</td>\n      <td>0.807229</td>\n      <td>0.923077</td>\n      <td>0.793103</td>\n      <td>0.760000</td>\n      <td>0.714286</td>\n      <td>0.407407</td>\n      <td>0.870968</td>\n      <td>0.748662</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.673300</td>\n      <td>0.691943</td>\n      <td>0.705882</td>\n      <td>0.843373</td>\n      <td>0.923077</td>\n      <td>0.793103</td>\n      <td>0.740000</td>\n      <td>0.714286</td>\n      <td>0.444444</td>\n      <td>0.903226</td>\n      <td>0.762329</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.673300</td>\n      <td>0.688272</td>\n      <td>0.705882</td>\n      <td>0.831325</td>\n      <td>0.923077</td>\n      <td>0.758621</td>\n      <td>0.760000</td>\n      <td>0.714286</td>\n      <td>0.407407</td>\n      <td>0.870968</td>\n      <td>0.752882</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.673300</td>\n      <td>0.698069</td>\n      <td>0.705882</td>\n      <td>0.855422</td>\n      <td>0.923077</td>\n      <td>0.793103</td>\n      <td>0.740000</td>\n      <td>0.714286</td>\n      <td>0.370370</td>\n      <td>0.870968</td>\n      <td>0.750452</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.673300</td>\n      <td>0.679150</td>\n      <td>0.705882</td>\n      <td>0.831325</td>\n      <td>0.923077</td>\n      <td>0.793103</td>\n      <td>0.760000</td>\n      <td>0.714286</td>\n      <td>0.407407</td>\n      <td>0.903226</td>\n      <td>0.758896</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.673300</td>\n      <td>0.675672</td>\n      <td>0.705882</td>\n      <td>0.831325</td>\n      <td>0.923077</td>\n      <td>0.793103</td>\n      <td>0.760000</td>\n      <td>0.714286</td>\n      <td>0.407407</td>\n      <td>0.903226</td>\n      <td>0.758896</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.493900</td>\n      <td>0.684275</td>\n      <td>0.705882</td>\n      <td>0.855422</td>\n      <td>0.923077</td>\n      <td>0.793103</td>\n      <td>0.740000</td>\n      <td>0.714286</td>\n      <td>0.407407</td>\n      <td>0.870968</td>\n      <td>0.756471</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.493900</td>\n      <td>0.690783</td>\n      <td>0.764706</td>\n      <td>0.879518</td>\n      <td>0.923077</td>\n      <td>0.793103</td>\n      <td>0.740000</td>\n      <td>0.714286</td>\n      <td>0.444444</td>\n      <td>0.870968</td>\n      <td>0.770728</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.493900</td>\n      <td>0.686281</td>\n      <td>0.705882</td>\n      <td>0.867470</td>\n      <td>0.923077</td>\n      <td>0.793103</td>\n      <td>0.740000</td>\n      <td>0.714286</td>\n      <td>0.407407</td>\n      <td>0.870968</td>\n      <td>0.758347</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.493900</td>\n      <td>0.680403</td>\n      <td>0.764706</td>\n      <td>0.819277</td>\n      <td>0.923077</td>\n      <td>0.793103</td>\n      <td>0.740000</td>\n      <td>0.714286</td>\n      <td>0.407407</td>\n      <td>0.903226</td>\n      <td>0.758032</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.493900</td>\n      <td>0.678087</td>\n      <td>0.764706</td>\n      <td>0.807229</td>\n      <td>0.923077</td>\n      <td>0.793103</td>\n      <td>0.760000</td>\n      <td>0.714286</td>\n      <td>0.407407</td>\n      <td>0.903226</td>\n      <td>0.756161</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.493900</td>\n      <td>0.677100</td>\n      <td>0.705882</td>\n      <td>0.855422</td>\n      <td>0.923077</td>\n      <td>0.758621</td>\n      <td>0.740000</td>\n      <td>0.714286</td>\n      <td>0.407407</td>\n      <td>0.870968</td>\n      <td>0.752002</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.493900</td>\n      <td>0.681437</td>\n      <td>0.764706</td>\n      <td>0.831325</td>\n      <td>0.923077</td>\n      <td>0.793103</td>\n      <td>0.740000</td>\n      <td>0.714286</td>\n      <td>0.407407</td>\n      <td>0.903226</td>\n      <td>0.760619</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.493900</td>\n      <td>0.679399</td>\n      <td>0.764706</td>\n      <td>0.855422</td>\n      <td>0.923077</td>\n      <td>0.793103</td>\n      <td>0.740000</td>\n      <td>0.714286</td>\n      <td>0.370370</td>\n      <td>0.903226</td>\n      <td>0.759369</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.493900</td>\n      <td>0.679242</td>\n      <td>0.764706</td>\n      <td>0.819277</td>\n      <td>0.923077</td>\n      <td>0.758621</td>\n      <td>0.760000</td>\n      <td>0.714286</td>\n      <td>0.370370</td>\n      <td>0.870968</td>\n      <td>0.746385</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.493900</td>\n      <td>0.679262</td>\n      <td>0.764706</td>\n      <td>0.843373</td>\n      <td>0.923077</td>\n      <td>0.793103</td>\n      <td>0.740000</td>\n      <td>0.714286</td>\n      <td>0.407407</td>\n      <td>0.870968</td>\n      <td>0.758418</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.493900</td>\n      <td>0.678954</td>\n      <td>0.764706</td>\n      <td>0.831325</td>\n      <td>0.923077</td>\n      <td>0.793103</td>\n      <td>0.740000</td>\n      <td>0.714286</td>\n      <td>0.407407</td>\n      <td>0.903226</td>\n      <td>0.759217</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.493900</td>\n      <td>0.677077</td>\n      <td>0.764706</td>\n      <td>0.819277</td>\n      <td>0.923077</td>\n      <td>0.758621</td>\n      <td>0.760000</td>\n      <td>0.714286</td>\n      <td>0.407407</td>\n      <td>0.870968</td>\n      <td>0.752299</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.424300</td>\n      <td>0.678280</td>\n      <td>0.764706</td>\n      <td>0.819277</td>\n      <td>0.923077</td>\n      <td>0.793103</td>\n      <td>0.740000</td>\n      <td>0.714286</td>\n      <td>0.407407</td>\n      <td>0.870968</td>\n      <td>0.752939</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2000, training_loss=0.7170685195922851, metrics={'train_runtime': 3372.9346, 'train_samples_per_second': 37.845, 'train_steps_per_second': 0.593, 'total_flos': 1.1639103081171936e+16, 'train_loss': 0.7170685195922851, 'epoch': 50.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"* Approach: Add a weighting to a CrossEntropy loss function","metadata":{}},{"cell_type":"code","source":"# We can see that we have a bad recall in 6 class --> it's hard to diverse other classes with sixth class, it could be much better\n# To resolve with this issue we should override compute_loss function in Trainer and set some weights in CrossEntropy loss function\ndef weighting():\n    counter = Counter(data['class'])\n    classSamples = np.array([counter[_class] for _class in label2ind])\n    totalSamples = sum(counter.values())\n    numClasses = 8\n    weights = torch.tensor(totalSamples / (numClasses * classSamples), dtype=torch.float)\n    return weights\n\nclass CustomTrainer(SFTTrainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        # Forward pass\n        outputs = model(**inputs)\n        logits = outputs.get('logits')\n        # Compute custom loss\n        if 'labels' in inputs:\n            loss = torch.nn.functional.cross_entropy(logits, inputs['labels'], weight=weighting().to('cuda'))\n        else:\n            loss = None\n        return (loss, outputs) if return_outputs else loss","metadata":{"execution":{"iopub.status.busy":"2024-03-20T11:01:27.365220Z","iopub.execute_input":"2024-03-20T11:01:27.365604Z","iopub.status.idle":"2024-03-20T11:01:27.375042Z","shell.execute_reply.started":"2024-03-20T11:01:27.365572Z","shell.execute_reply":"2024-03-20T11:01:27.374101Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"trainer = CustomTrainer(\n    model=model,\n    args=training_arguments,\n    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n    train_dataset=trainDataset,\n    eval_dataset=valDataset,\n    compute_metrics=compute_metrics,\n    peft_config=peft_config,\n    dataset_text_field='text'\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:08:30.451801Z","iopub.execute_input":"2024-03-20T07:08:30.452070Z","iopub.status.idle":"2024-03-20T07:08:30.818764Z","shell.execute_reply.started":"2024-03-20T07:08:30.452048Z","shell.execute_reply":"2024-03-20T07:08:30.817763Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Getting result a little bit better (F1 score - 0.825) than in previous time, but it's not enough, it seems that current model\n# have not enough russian embeddings (only 50k) in comparison with W2V, which has 300k+. Let's choose another model and train it as in current time\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-03-18T18:56:46.255599Z","iopub.execute_input":"2024-03-18T18:56:46.256286Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7932' max='8000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7932/8000 54:26 < 00:28, 2.43 it/s, Epoch 49.57/50]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Class 0 Recall</th>\n      <th>Class 1 Recall</th>\n      <th>Class 2 Recall</th>\n      <th>Class 3 Recall</th>\n      <th>Class 4 Recall</th>\n      <th>Class 5 Recall</th>\n      <th>Class 6 Recall</th>\n      <th>Class 7 Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.819069</td>\n      <td>0.000000</td>\n      <td>0.698795</td>\n      <td>0.192308</td>\n      <td>0.482759</td>\n      <td>0.100000</td>\n      <td>0.047619</td>\n      <td>0.296296</td>\n      <td>0.548387</td>\n      <td>0.278999</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>1.420957</td>\n      <td>0.647059</td>\n      <td>0.108434</td>\n      <td>0.807692</td>\n      <td>0.793103</td>\n      <td>0.280000</td>\n      <td>0.380952</td>\n      <td>0.222222</td>\n      <td>0.612903</td>\n      <td>0.410079</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>1.262708</td>\n      <td>0.705882</td>\n      <td>0.132530</td>\n      <td>0.807692</td>\n      <td>0.724138</td>\n      <td>0.480000</td>\n      <td>0.619048</td>\n      <td>0.333333</td>\n      <td>0.774194</td>\n      <td>0.509880</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.604200</td>\n      <td>1.056074</td>\n      <td>0.823529</td>\n      <td>0.469880</td>\n      <td>0.884615</td>\n      <td>0.620690</td>\n      <td>0.560000</td>\n      <td>0.666667</td>\n      <td>0.259259</td>\n      <td>0.870968</td>\n      <td>0.599357</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.604200</td>\n      <td>0.958433</td>\n      <td>0.647059</td>\n      <td>0.542169</td>\n      <td>0.884615</td>\n      <td>0.586207</td>\n      <td>0.660000</td>\n      <td>0.666667</td>\n      <td>0.481481</td>\n      <td>0.806452</td>\n      <td>0.636563</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.604200</td>\n      <td>0.870012</td>\n      <td>0.470588</td>\n      <td>0.626506</td>\n      <td>0.846154</td>\n      <td>0.620690</td>\n      <td>0.640000</td>\n      <td>0.714286</td>\n      <td>0.592593</td>\n      <td>0.935484</td>\n      <td>0.675330</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.922500</td>\n      <td>0.818124</td>\n      <td>0.470588</td>\n      <td>0.686747</td>\n      <td>0.923077</td>\n      <td>0.655172</td>\n      <td>0.700000</td>\n      <td>0.714286</td>\n      <td>0.555556</td>\n      <td>0.838710</td>\n      <td>0.687249</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.922500</td>\n      <td>0.763824</td>\n      <td>0.941176</td>\n      <td>0.542169</td>\n      <td>0.961538</td>\n      <td>0.655172</td>\n      <td>0.700000</td>\n      <td>0.714286</td>\n      <td>0.555556</td>\n      <td>0.967742</td>\n      <td>0.710897</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.922500</td>\n      <td>0.721469</td>\n      <td>0.823529</td>\n      <td>0.638554</td>\n      <td>0.961538</td>\n      <td>0.620690</td>\n      <td>0.720000</td>\n      <td>0.714286</td>\n      <td>0.666667</td>\n      <td>0.838710</td>\n      <td>0.721420</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.654100</td>\n      <td>0.696658</td>\n      <td>0.705882</td>\n      <td>0.662651</td>\n      <td>0.961538</td>\n      <td>0.758621</td>\n      <td>0.740000</td>\n      <td>0.714286</td>\n      <td>0.703704</td>\n      <td>0.903226</td>\n      <td>0.751004</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.654100</td>\n      <td>0.700293</td>\n      <td>0.941176</td>\n      <td>0.626506</td>\n      <td>0.961538</td>\n      <td>0.689655</td>\n      <td>0.680000</td>\n      <td>0.714286</td>\n      <td>0.629630</td>\n      <td>0.935484</td>\n      <td>0.737670</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.654100</td>\n      <td>0.682195</td>\n      <td>0.941176</td>\n      <td>0.662651</td>\n      <td>0.923077</td>\n      <td>0.758621</td>\n      <td>0.800000</td>\n      <td>0.714286</td>\n      <td>0.592593</td>\n      <td>0.870968</td>\n      <td>0.759215</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.519600</td>\n      <td>0.710637</td>\n      <td>0.941176</td>\n      <td>0.710843</td>\n      <td>0.961538</td>\n      <td>0.724138</td>\n      <td>0.780000</td>\n      <td>0.714286</td>\n      <td>0.407407</td>\n      <td>0.903226</td>\n      <td>0.746818</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.519600</td>\n      <td>0.673223</td>\n      <td>0.941176</td>\n      <td>0.662651</td>\n      <td>1.000000</td>\n      <td>0.793103</td>\n      <td>0.800000</td>\n      <td>0.714286</td>\n      <td>0.518519</td>\n      <td>0.903226</td>\n      <td>0.763541</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.519600</td>\n      <td>0.698081</td>\n      <td>0.882353</td>\n      <td>0.638554</td>\n      <td>1.000000</td>\n      <td>0.793103</td>\n      <td>0.680000</td>\n      <td>0.714286</td>\n      <td>0.481481</td>\n      <td>0.935484</td>\n      <td>0.738635</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.410800</td>\n      <td>0.691965</td>\n      <td>0.882353</td>\n      <td>0.650602</td>\n      <td>1.000000</td>\n      <td>0.758621</td>\n      <td>0.840000</td>\n      <td>0.714286</td>\n      <td>0.555556</td>\n      <td>0.903226</td>\n      <td>0.764346</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.410800</td>\n      <td>0.707641</td>\n      <td>0.941176</td>\n      <td>0.710843</td>\n      <td>1.000000</td>\n      <td>0.793103</td>\n      <td>0.740000</td>\n      <td>0.714286</td>\n      <td>0.444444</td>\n      <td>0.935484</td>\n      <td>0.761223</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.410800</td>\n      <td>0.714707</td>\n      <td>0.823529</td>\n      <td>0.722892</td>\n      <td>1.000000</td>\n      <td>0.689655</td>\n      <td>0.800000</td>\n      <td>0.714286</td>\n      <td>0.592593</td>\n      <td>0.903226</td>\n      <td>0.768713</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.339100</td>\n      <td>0.685577</td>\n      <td>0.823529</td>\n      <td>0.698795</td>\n      <td>1.000000</td>\n      <td>0.793103</td>\n      <td>0.780000</td>\n      <td>0.714286</td>\n      <td>0.629630</td>\n      <td>0.870968</td>\n      <td>0.776328</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.339100</td>\n      <td>0.683825</td>\n      <td>0.882353</td>\n      <td>0.698795</td>\n      <td>1.000000</td>\n      <td>0.793103</td>\n      <td>0.800000</td>\n      <td>0.761905</td>\n      <td>0.592593</td>\n      <td>0.935484</td>\n      <td>0.792252</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.339100</td>\n      <td>0.695728</td>\n      <td>0.941176</td>\n      <td>0.626506</td>\n      <td>1.000000</td>\n      <td>0.793103</td>\n      <td>0.760000</td>\n      <td>0.714286</td>\n      <td>0.629630</td>\n      <td>0.903226</td>\n      <td>0.770482</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.299700</td>\n      <td>0.719451</td>\n      <td>0.882353</td>\n      <td>0.722892</td>\n      <td>1.000000</td>\n      <td>0.827586</td>\n      <td>0.760000</td>\n      <td>0.714286</td>\n      <td>0.481481</td>\n      <td>0.935484</td>\n      <td>0.771257</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.299700</td>\n      <td>0.729652</td>\n      <td>0.941176</td>\n      <td>0.698795</td>\n      <td>1.000000</td>\n      <td>0.827586</td>\n      <td>0.720000</td>\n      <td>0.714286</td>\n      <td>0.555556</td>\n      <td>0.935484</td>\n      <td>0.770730</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.299700</td>\n      <td>0.723476</td>\n      <td>0.882353</td>\n      <td>0.674699</td>\n      <td>1.000000</td>\n      <td>0.827586</td>\n      <td>0.760000</td>\n      <td>0.714286</td>\n      <td>0.629630</td>\n      <td>0.935484</td>\n      <td>0.780595</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.256700</td>\n      <td>0.725594</td>\n      <td>0.882353</td>\n      <td>0.698795</td>\n      <td>1.000000</td>\n      <td>0.793103</td>\n      <td>0.740000</td>\n      <td>0.714286</td>\n      <td>0.592593</td>\n      <td>0.935484</td>\n      <td>0.774329</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.256700</td>\n      <td>0.724944</td>\n      <td>0.882353</td>\n      <td>0.722892</td>\n      <td>1.000000</td>\n      <td>0.827586</td>\n      <td>0.760000</td>\n      <td>0.761905</td>\n      <td>0.592593</td>\n      <td>0.870968</td>\n      <td>0.787085</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.256700</td>\n      <td>0.737404</td>\n      <td>0.823529</td>\n      <td>0.734940</td>\n      <td>1.000000</td>\n      <td>0.827586</td>\n      <td>0.760000</td>\n      <td>0.761905</td>\n      <td>0.592593</td>\n      <td>0.870968</td>\n      <td>0.787993</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.256700</td>\n      <td>0.738720</td>\n      <td>0.823529</td>\n      <td>0.746988</td>\n      <td>1.000000</td>\n      <td>0.827586</td>\n      <td>0.780000</td>\n      <td>0.761905</td>\n      <td>0.592593</td>\n      <td>0.903226</td>\n      <td>0.797131</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.213600</td>\n      <td>0.773250</td>\n      <td>0.882353</td>\n      <td>0.759036</td>\n      <td>1.000000</td>\n      <td>0.827586</td>\n      <td>0.740000</td>\n      <td>0.761905</td>\n      <td>0.518519</td>\n      <td>0.935484</td>\n      <td>0.787830</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.213600</td>\n      <td>0.736564</td>\n      <td>0.882353</td>\n      <td>0.722892</td>\n      <td>1.000000</td>\n      <td>0.827586</td>\n      <td>0.740000</td>\n      <td>0.809524</td>\n      <td>0.592593</td>\n      <td>0.903226</td>\n      <td>0.794638</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.213600</td>\n      <td>0.731303</td>\n      <td>0.882353</td>\n      <td>0.710843</td>\n      <td>1.000000</td>\n      <td>0.793103</td>\n      <td>0.760000</td>\n      <td>0.857143</td>\n      <td>0.592593</td>\n      <td>0.903226</td>\n      <td>0.796017</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.191200</td>\n      <td>0.761529</td>\n      <td>0.882353</td>\n      <td>0.771084</td>\n      <td>1.000000</td>\n      <td>0.827586</td>\n      <td>0.780000</td>\n      <td>0.761905</td>\n      <td>0.592593</td>\n      <td>0.935484</td>\n      <td>0.807158</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.191200</td>\n      <td>0.774766</td>\n      <td>0.882353</td>\n      <td>0.807229</td>\n      <td>1.000000</td>\n      <td>0.827586</td>\n      <td>0.800000</td>\n      <td>0.809524</td>\n      <td>0.592593</td>\n      <td>0.935484</td>\n      <td>0.824943</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.191200</td>\n      <td>0.768009</td>\n      <td>0.823529</td>\n      <td>0.746988</td>\n      <td>1.000000</td>\n      <td>0.827586</td>\n      <td>0.820000</td>\n      <td>0.809524</td>\n      <td>0.592593</td>\n      <td>0.935484</td>\n      <td>0.812036</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.161800</td>\n      <td>0.843399</td>\n      <td>0.882353</td>\n      <td>0.807229</td>\n      <td>1.000000</td>\n      <td>0.793103</td>\n      <td>0.820000</td>\n      <td>0.809524</td>\n      <td>0.481481</td>\n      <td>0.903226</td>\n      <td>0.814239</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.161800</td>\n      <td>0.772588</td>\n      <td>0.823529</td>\n      <td>0.771084</td>\n      <td>1.000000</td>\n      <td>0.793103</td>\n      <td>0.780000</td>\n      <td>0.857143</td>\n      <td>0.592593</td>\n      <td>0.870968</td>\n      <td>0.805335</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.161800</td>\n      <td>0.755074</td>\n      <td>0.941176</td>\n      <td>0.734940</td>\n      <td>1.000000</td>\n      <td>0.827586</td>\n      <td>0.800000</td>\n      <td>0.857143</td>\n      <td>0.592593</td>\n      <td>0.935484</td>\n      <td>0.820478</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.148200</td>\n      <td>0.770861</td>\n      <td>0.882353</td>\n      <td>0.771084</td>\n      <td>1.000000</td>\n      <td>0.827586</td>\n      <td>0.800000</td>\n      <td>0.857143</td>\n      <td>0.592593</td>\n      <td>0.935484</td>\n      <td>0.822107</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.148200</td>\n      <td>0.820610</td>\n      <td>0.941176</td>\n      <td>0.795181</td>\n      <td>1.000000</td>\n      <td>0.793103</td>\n      <td>0.760000</td>\n      <td>0.857143</td>\n      <td>0.555556</td>\n      <td>0.935484</td>\n      <td>0.809314</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.148200</td>\n      <td>0.790709</td>\n      <td>0.882353</td>\n      <td>0.734940</td>\n      <td>1.000000</td>\n      <td>0.827586</td>\n      <td>0.820000</td>\n      <td>0.857143</td>\n      <td>0.592593</td>\n      <td>0.935484</td>\n      <td>0.819372</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.138100</td>\n      <td>0.807152</td>\n      <td>0.882353</td>\n      <td>0.771084</td>\n      <td>1.000000</td>\n      <td>0.827586</td>\n      <td>0.820000</td>\n      <td>0.857143</td>\n      <td>0.555556</td>\n      <td>0.903226</td>\n      <td>0.818908</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.138100</td>\n      <td>0.798257</td>\n      <td>0.882353</td>\n      <td>0.771084</td>\n      <td>1.000000</td>\n      <td>0.827586</td>\n      <td>0.820000</td>\n      <td>0.809524</td>\n      <td>0.592593</td>\n      <td>0.870968</td>\n      <td>0.814690</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.138100</td>\n      <td>0.830874</td>\n      <td>0.882353</td>\n      <td>0.795181</td>\n      <td>1.000000</td>\n      <td>0.827586</td>\n      <td>0.820000</td>\n      <td>0.857143</td>\n      <td>0.555556</td>\n      <td>0.870968</td>\n      <td>0.820163</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.116900</td>\n      <td>0.803332</td>\n      <td>0.882353</td>\n      <td>0.746988</td>\n      <td>1.000000</td>\n      <td>0.827586</td>\n      <td>0.820000</td>\n      <td>0.857143</td>\n      <td>0.592593</td>\n      <td>0.935484</td>\n      <td>0.821347</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.116900</td>\n      <td>0.828062</td>\n      <td>0.882353</td>\n      <td>0.795181</td>\n      <td>1.000000</td>\n      <td>0.827586</td>\n      <td>0.820000</td>\n      <td>0.857143</td>\n      <td>0.555556</td>\n      <td>0.935484</td>\n      <td>0.826903</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.116900</td>\n      <td>0.822824</td>\n      <td>0.882353</td>\n      <td>0.759036</td>\n      <td>1.000000</td>\n      <td>0.827586</td>\n      <td>0.800000</td>\n      <td>0.857143</td>\n      <td>0.592593</td>\n      <td>0.935484</td>\n      <td>0.820139</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.110800</td>\n      <td>0.823178</td>\n      <td>0.882353</td>\n      <td>0.771084</td>\n      <td>1.000000</td>\n      <td>0.827586</td>\n      <td>0.820000</td>\n      <td>0.857143</td>\n      <td>0.592593</td>\n      <td>0.935484</td>\n      <td>0.825775</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.110800</td>\n      <td>0.824414</td>\n      <td>0.882353</td>\n      <td>0.771084</td>\n      <td>1.000000</td>\n      <td>0.827586</td>\n      <td>0.800000</td>\n      <td>0.857143</td>\n      <td>0.592593</td>\n      <td>0.935484</td>\n      <td>0.824115</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.110800</td>\n      <td>0.816605</td>\n      <td>0.882353</td>\n      <td>0.759036</td>\n      <td>1.000000</td>\n      <td>0.827586</td>\n      <td>0.800000</td>\n      <td>0.857143</td>\n      <td>0.592593</td>\n      <td>0.935484</td>\n      <td>0.821609</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"markdown","source":"* Approach: Trying the model with more embeddings","metadata":{}},{"cell_type":"code","source":"yamodel_name = 'AlexKay/xlm-roberta-large-qa-multilingual-finedtuned-ru'\nyamodel = AutoModelForSequenceClassification.from_pretrained(yamodel_name, num_labels=8)\ntokenizer = AutoTokenizer.from_pretrained(yamodel_name)\n\nfor (name, module) in yamodel.roberta.named_modules():\n    if name.startswith('embeddings'):\n        pass\n    else:\n        for parameter in module.parameters():\n            parameter.requires_grad = False\nyamodel","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:08:30.820164Z","iopub.execute_input":"2024-03-20T07:08:30.820531Z","iopub.status.idle":"2024-03-20T07:09:27.175626Z","shell.execute_reply.started":"2024-03-20T07:08:30.820498Z","shell.execute_reply":"2024-03-20T07:09:27.174726Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/781 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62f8c2396c084bd087fd732b94054d51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93453046d9b94eb8bc85d25b6b8061c8"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at AlexKay/xlm-roberta-large-qa-multilingual-finedtuned-ru and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/516 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29441813700e4db99b2990e03aab3e13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c4fddfe5fde4e1ab84883ac4900285f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8503ecd8dd8a40b9b992440283463dcc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ff531eadb314167aa766c6964639cbe"}},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"XLMRobertaForSequenceClassification(\n  (roberta): XLMRobertaModel(\n    (embeddings): XLMRobertaEmbeddings(\n      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 1024)\n      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): XLMRobertaEncoder(\n      (layer): ModuleList(\n        (0-23): 24 x XLMRobertaLayer(\n          (attention): XLMRobertaAttention(\n            (self): XLMRobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): XLMRobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): XLMRobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): XLMRobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): XLMRobertaClassificationHead(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=1024, out_features=8, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"# 0.85 F1 score end then model has been overfitted, we need to remove some layers \n\ntrainDataset = Dataset(x_train, y_train, label2ind, tokenizer)\nvalDataset = Dataset(x_val, y_val, label2ind, tokenizer)\n\npeft_config = LoraConfig(\n    lora_alpha=8,\n    lora_dropout=0.05,\n    r=8,\n    bias=\"none\",\n    target_modules = [\"query\", \"key\", \"value\", \"dense\"],\n    task_type=\"SEQ_CLS\",\n)\n\ntraining_arguments = TrainingArguments(\n    output_dir=\"logs\",\n    num_train_epochs=50,\n    per_device_train_batch_size=4,\n    save_strategy='no',\n    evaluation_strategy=\"epoch\"\n)\n\ntrainer = CustomTrainer(\n    model=yamodel,\n    args=training_arguments,\n    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n    train_dataset=trainDataset,\n    eval_dataset=valDataset,\n    compute_metrics=compute_metrics,\n    peft_config=peft_config,\n    dataset_text_field='text'\n)\n\ntrainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Approach: As previous encoder with more embeddings made a profit and we see an overfitting, which may be caused by huge size of the model, we can take FastText300 russian embeddings and train our model with the first approach on 12 encoder layers","metadata":{}},{"cell_type":"code","source":"# Download W2V embeddings\n\nembed_lookup = KeyedVectors.load_word2vec_format('/kaggle/input/embeddings/cc.ru.300.vec', binary=False) # [600K, 300]","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:09:27.176875Z","iopub.execute_input":"2024-03-20T07:09:27.177974Z","iopub.status.idle":"2024-03-20T07:12:14.814675Z","shell.execute_reply.started":"2024-03-20T07:09:27.177946Z","shell.execute_reply":"2024-03-20T07:12:14.813892Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Generate random embeddings for utils tokens\n\ndef random_embedding():\n    return np.random.normal(loc=0.0, scale=0.1, size=300).astype('float32')\n\nspecial_tokens_embeds = {\n    'bos': random_embedding(),\n    'eos': random_embedding(),\n    'unk': random_embedding(),\n    'pad': random_embedding()\n}\n\n# Replace current and add missing embeddings for utils tokens\n\nfor token in special_tokens_embeds.keys():\n    embed_lookup[token] = special_tokens_embeds[token]\n\n\nspecial_tokens_ids = {\n    'bos': embed_lookup.get_index('bos'),\n    'eos': embed_lookup.get_index('eos'),\n    'unk': embed_lookup.get_index('unk'),\n    'pad': embed_lookup.get_index('pad')\n}","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:12:14.815860Z","iopub.execute_input":"2024-03-20T07:12:14.816138Z","iopub.status.idle":"2024-03-20T07:12:15.915738Z","shell.execute_reply.started":"2024-03-20T07:12:14.816113Z","shell.execute_reply":"2024-03-20T07:12:15.914902Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Define Dataset inherited from original one above. We need this as now we are using W2V embeddings\n# and token ids differs from tokenizer's of roberta model\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nclass w2vDataset(Dataset):\n    \n    def __init__(self, x, y, label2ind, embed_lookup, special_tokens_ids):\n        super().__init__(x, y, label2ind)\n        self.embed_lookup = embed_lookup\n        self.special_tokens_ids = special_tokens_ids\n    \n    def prepare_example(self, text):\n        text = re.sub(patterns, ' ', text)\n        tokens = []\n        for token in text.split():\n            token = token.strip()\n            token = morph.normal_forms(token)[0]\n            tokens.append(token)\n        return self._tokenizer(' '.join(tokens))\n    \n    def _tokenizer(self, s: str):\n        input_ids = [self.special_tokens_ids['bos']]\n        for token in s.split():\n\n            if token in embed_lookup:\n                input_ids.append(self.embed_lookup.get_index(token))\n            else:\n                input_ids.append(self.special_tokens_ids['unk'])\n        input_ids.append(self.special_tokens_ids['eos'])\n\n        dictionary = {\n            'input_ids': input_ids,\n            'attention_mask': [1] * len(input_ids)\n        }\n        return dictionary\n    \n# And collator_fn\ndef collator_fn(batch):\n        \n    def len_ids(sample):\n        return len(sample['input_ids'])\n    maxlen = max(list(map(len_ids, batch)))\n    \n    input_ids = []\n    attention_mask = []\n    labels = []\n    for sample in batch:\n        gap = maxlen - len(sample['input_ids'])\n        if len(sample['input_ids']) == maxlen:\n            input_ids.append(sample['input_ids'])\n            attention_mask.append(sample['attention_mask'])\n        \n        else:\n            input_ids.append(sample['input_ids'] + [special_tokens_ids['pad']] * gap)\n            attention_mask.append(sample['attention_mask'] + [0] * gap)\n            \n        if 'label' in sample: labels.append(sample['label'])\n    \n    dictionary = {\n        'input_ids': torch.tensor(input_ids).to(device),\n        'attention_mask': torch.tensor(attention_mask).to(device),\n    }\n    \n    if 'label' in sample: dictionary['labels']: torch.tensor(labels).to(device)\n    \n    return dictionary","metadata":{"execution":{"iopub.status.busy":"2024-03-20T11:04:28.640594Z","iopub.execute_input":"2024-03-20T11:04:28.641591Z","iopub.status.idle":"2024-03-20T11:04:28.657448Z","shell.execute_reply.started":"2024-03-20T11:04:28.641554Z","shell.execute_reply":"2024-03-20T11:04:28.656155Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModel, AutoConfig\n\n# Downloading a config for the first approch model\nconfig = AutoConfig.from_pretrained('blinoff/roberta-base-russian-v0')\nconfig.hidden_size=300\nconfig.num_labels=8\n\n\nlastmodel = AutoModelForSequenceClassification.from_config(config)\n\n# Replace Embedding layer\nweight = torch.FloatTensor(embed_lookup.vectors)\nnewEmbedding = nn.Embedding.from_pretrained(weight)\nlastmodel.roberta.embeddings.word_embeddings = newEmbedding\n\nlastmodel","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:57:53.425522Z","iopub.execute_input":"2024-03-20T07:57:53.426369Z","iopub.status.idle":"2024-03-20T07:57:54.358450Z","shell.execute_reply.started":"2024-03-20T07:57:53.426335Z","shell.execute_reply":"2024-03-20T07:57:54.357405Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(600002, 300)\n      (position_embeddings): Embedding(514, 300, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 300)\n      (LayerNorm): LayerNorm((300,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=300, out_features=300, bias=True)\n              (key): Linear(in_features=300, out_features=300, bias=True)\n              (value): Linear(in_features=300, out_features=300, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=300, out_features=300, bias=True)\n              (LayerNorm): LayerNorm((300,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=300, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=300, bias=True)\n            (LayerNorm): LayerNorm((300,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=300, out_features=300, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=300, out_features=8, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Creating Datasets\n\ntrainDataset = w2vDataset(x_train, y_train, label2ind, embed_lookup, special_tokens_ids)\nvalDataset = w2vDataset(x_val, y_val, label2ind, embed_lookup, special_tokens_ids)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T10:24:54.478177Z","iopub.execute_input":"2024-03-20T10:24:54.478919Z","iopub.status.idle":"2024-03-20T10:24:54.486831Z","shell.execute_reply.started":"2024-03-20T10:24:54.478883Z","shell.execute_reply":"2024-03-20T10:24:54.485574Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(p):\n    preds = np.argmax(p.predictions, axis=1)\n    labels = p.label_ids\n\n    # Calculate recall for each class\n    recalls = recall_score(labels, preds, average=None, labels=[0, 1, 2, 3, 4, 5, 6, 7]) # Adjust labels based on your classes\n\n    # Create a dictionary to store recall for each class\n    results = {}\n    for i, recall in enumerate(recalls):\n        results[f'class_{i}_recall'] = recall\n    \n    f1 = f1_score(labels, preds, average='macro')\n    results['F1'] = f1\n\n    return results\n\ntraining_arguments = TrainingArguments(\n    output_dir=\"logs\",\n    num_train_epochs=50,\n    per_device_train_batch_size=80,\n    save_strategy='no',\n    evaluation_strategy=\"epoch\"\n)\n\ntrainer = CustomTrainer(\n    model=lastmodel,\n    args=training_arguments,\n    data_collator=collator_fn,\n    train_dataset=trainDataset,\n    eval_dataset=valDataset,\n    compute_metrics=compute_metrics,\n    dataset_text_field='text'\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:12:16.885852Z","iopub.execute_input":"2024-03-20T07:12:16.886632Z","iopub.status.idle":"2024-03-20T07:12:17.374316Z","shell.execute_reply.started":"2024-03-20T07:12:16.886599Z","shell.execute_reply":"2024-03-20T07:12:17.373505Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# The best results without weight decay is 0.877 on step 43 (Better than in base second approach)\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-03-19T21:58:26.323795Z","iopub.execute_input":"2024-03-19T21:58:26.324589Z","iopub.status.idle":"2024-03-19T22:35:02.974043Z","shell.execute_reply.started":"2024-03-19T21:58:26.324553Z","shell.execute_reply":"2024-03-19T22:35:02.972912Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240319_215854-n68yjdys</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/lamas/huggingface/runs/n68yjdys' target=\"_blank\">generous-rain-17</a></strong> to <a href='https://wandb.ai/lamas/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/lamas/huggingface' target=\"_blank\">https://wandb.ai/lamas/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/lamas/huggingface/runs/n68yjdys' target=\"_blank\">https://wandb.ai/lamas/huggingface/runs/n68yjdys</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2000/2000 35:34, Epoch 50/50]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Class 0 Recall</th>\n      <th>Class 1 Recall</th>\n      <th>Class 2 Recall</th>\n      <th>Class 3 Recall</th>\n      <th>Class 4 Recall</th>\n      <th>Class 5 Recall</th>\n      <th>Class 6 Recall</th>\n      <th>Class 7 Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>2.000194</td>\n      <td>0.882353</td>\n      <td>0.000000</td>\n      <td>0.076923</td>\n      <td>0.000000</td>\n      <td>0.020000</td>\n      <td>0.333333</td>\n      <td>0.000000</td>\n      <td>0.483871</td>\n      <td>0.129922</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>1.616782</td>\n      <td>0.647059</td>\n      <td>0.204819</td>\n      <td>0.769231</td>\n      <td>0.310345</td>\n      <td>0.300000</td>\n      <td>0.190476</td>\n      <td>0.000000</td>\n      <td>0.677419</td>\n      <td>0.328717</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>1.369914</td>\n      <td>0.882353</td>\n      <td>0.072289</td>\n      <td>0.923077</td>\n      <td>0.793103</td>\n      <td>0.420000</td>\n      <td>0.523810</td>\n      <td>0.222222</td>\n      <td>0.774194</td>\n      <td>0.482968</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>1.079227</td>\n      <td>1.000000</td>\n      <td>0.253012</td>\n      <td>0.961538</td>\n      <td>0.827586</td>\n      <td>0.440000</td>\n      <td>0.666667</td>\n      <td>0.333333</td>\n      <td>0.903226</td>\n      <td>0.592149</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>0.862583</td>\n      <td>0.882353</td>\n      <td>0.493976</td>\n      <td>0.961538</td>\n      <td>0.896552</td>\n      <td>0.660000</td>\n      <td>0.761905</td>\n      <td>0.444444</td>\n      <td>0.903226</td>\n      <td>0.711493</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>No log</td>\n      <td>0.751134</td>\n      <td>0.941176</td>\n      <td>0.638554</td>\n      <td>1.000000</td>\n      <td>0.827586</td>\n      <td>0.700000</td>\n      <td>0.761905</td>\n      <td>0.555556</td>\n      <td>0.903226</td>\n      <td>0.757765</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>No log</td>\n      <td>0.692155</td>\n      <td>0.882353</td>\n      <td>0.710843</td>\n      <td>1.000000</td>\n      <td>0.827586</td>\n      <td>0.680000</td>\n      <td>0.666667</td>\n      <td>0.592593</td>\n      <td>0.935484</td>\n      <td>0.765316</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>No log</td>\n      <td>0.671968</td>\n      <td>0.882353</td>\n      <td>0.662651</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.660000</td>\n      <td>0.619048</td>\n      <td>0.592593</td>\n      <td>0.903226</td>\n      <td>0.743471</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>No log</td>\n      <td>0.601805</td>\n      <td>0.882353</td>\n      <td>0.602410</td>\n      <td>1.000000</td>\n      <td>0.793103</td>\n      <td>0.760000</td>\n      <td>0.714286</td>\n      <td>0.666667</td>\n      <td>0.935484</td>\n      <td>0.759700</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>No log</td>\n      <td>0.584327</td>\n      <td>0.882353</td>\n      <td>0.783133</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.720000</td>\n      <td>0.761905</td>\n      <td>0.592593</td>\n      <td>0.903226</td>\n      <td>0.800126</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>No log</td>\n      <td>0.571315</td>\n      <td>0.882353</td>\n      <td>0.819277</td>\n      <td>1.000000</td>\n      <td>0.896552</td>\n      <td>0.700000</td>\n      <td>0.761905</td>\n      <td>0.592593</td>\n      <td>0.935484</td>\n      <td>0.811926</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>No log</td>\n      <td>0.564559</td>\n      <td>0.882353</td>\n      <td>0.734940</td>\n      <td>1.000000</td>\n      <td>0.896552</td>\n      <td>0.700000</td>\n      <td>0.714286</td>\n      <td>0.629630</td>\n      <td>0.935484</td>\n      <td>0.795230</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.931300</td>\n      <td>0.539949</td>\n      <td>0.882353</td>\n      <td>0.674699</td>\n      <td>1.000000</td>\n      <td>0.896552</td>\n      <td>0.760000</td>\n      <td>0.761905</td>\n      <td>0.629630</td>\n      <td>0.935484</td>\n      <td>0.791724</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.931300</td>\n      <td>0.537170</td>\n      <td>0.882353</td>\n      <td>0.771084</td>\n      <td>1.000000</td>\n      <td>0.827586</td>\n      <td>0.720000</td>\n      <td>0.809524</td>\n      <td>0.592593</td>\n      <td>0.935484</td>\n      <td>0.803920</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.931300</td>\n      <td>0.535077</td>\n      <td>0.882353</td>\n      <td>0.602410</td>\n      <td>1.000000</td>\n      <td>0.931034</td>\n      <td>0.720000</td>\n      <td>0.714286</td>\n      <td>0.740741</td>\n      <td>0.903226</td>\n      <td>0.780233</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.931300</td>\n      <td>0.520293</td>\n      <td>0.941176</td>\n      <td>0.734940</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.780000</td>\n      <td>0.714286</td>\n      <td>0.592593</td>\n      <td>0.935484</td>\n      <td>0.805874</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.931300</td>\n      <td>0.546001</td>\n      <td>0.882353</td>\n      <td>0.795181</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.760000</td>\n      <td>0.761905</td>\n      <td>0.592593</td>\n      <td>0.935484</td>\n      <td>0.819268</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.931300</td>\n      <td>0.496519</td>\n      <td>0.941176</td>\n      <td>0.795181</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.740000</td>\n      <td>0.857143</td>\n      <td>0.629630</td>\n      <td>0.935484</td>\n      <td>0.829187</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.931300</td>\n      <td>0.469633</td>\n      <td>0.941176</td>\n      <td>0.771084</td>\n      <td>1.000000</td>\n      <td>0.793103</td>\n      <td>0.780000</td>\n      <td>0.761905</td>\n      <td>0.703704</td>\n      <td>0.935484</td>\n      <td>0.822953</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.931300</td>\n      <td>0.527172</td>\n      <td>0.941176</td>\n      <td>0.807229</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.720000</td>\n      <td>0.619048</td>\n      <td>0.666667</td>\n      <td>0.935484</td>\n      <td>0.813299</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.931300</td>\n      <td>0.464046</td>\n      <td>0.941176</td>\n      <td>0.710843</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.780000</td>\n      <td>0.714286</td>\n      <td>0.777778</td>\n      <td>0.903226</td>\n      <td>0.813806</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.931300</td>\n      <td>0.465836</td>\n      <td>0.882353</td>\n      <td>0.855422</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.720000</td>\n      <td>0.666667</td>\n      <td>0.740741</td>\n      <td>0.903226</td>\n      <td>0.831276</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.931300</td>\n      <td>0.505773</td>\n      <td>0.882353</td>\n      <td>0.915663</td>\n      <td>1.000000</td>\n      <td>0.896552</td>\n      <td>0.760000</td>\n      <td>0.761905</td>\n      <td>0.666667</td>\n      <td>0.838710</td>\n      <td>0.851834</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.931300</td>\n      <td>0.497480</td>\n      <td>0.941176</td>\n      <td>0.710843</td>\n      <td>1.000000</td>\n      <td>0.896552</td>\n      <td>0.780000</td>\n      <td>0.761905</td>\n      <td>0.740741</td>\n      <td>0.903226</td>\n      <td>0.820334</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.261900</td>\n      <td>0.517586</td>\n      <td>0.941176</td>\n      <td>0.855422</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.740000</td>\n      <td>0.666667</td>\n      <td>0.629630</td>\n      <td>0.903226</td>\n      <td>0.815639</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.261900</td>\n      <td>0.523870</td>\n      <td>0.823529</td>\n      <td>0.891566</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.760000</td>\n      <td>0.809524</td>\n      <td>0.666667</td>\n      <td>0.903226</td>\n      <td>0.846378</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.261900</td>\n      <td>0.511418</td>\n      <td>0.882353</td>\n      <td>0.903614</td>\n      <td>1.000000</td>\n      <td>0.896552</td>\n      <td>0.760000</td>\n      <td>0.952381</td>\n      <td>0.703704</td>\n      <td>0.806452</td>\n      <td>0.870263</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.261900</td>\n      <td>0.490603</td>\n      <td>0.882353</td>\n      <td>0.795181</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.840000</td>\n      <td>0.904762</td>\n      <td>0.629630</td>\n      <td>0.903226</td>\n      <td>0.848201</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.261900</td>\n      <td>0.542878</td>\n      <td>0.823529</td>\n      <td>0.746988</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.780000</td>\n      <td>0.904762</td>\n      <td>0.666667</td>\n      <td>0.903226</td>\n      <td>0.830308</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.261900</td>\n      <td>0.564129</td>\n      <td>0.941176</td>\n      <td>0.879518</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.720000</td>\n      <td>0.714286</td>\n      <td>0.703704</td>\n      <td>0.838710</td>\n      <td>0.831593</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.261900</td>\n      <td>0.534795</td>\n      <td>0.882353</td>\n      <td>0.915663</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.740000</td>\n      <td>0.857143</td>\n      <td>0.703704</td>\n      <td>0.838710</td>\n      <td>0.857747</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.261900</td>\n      <td>0.531056</td>\n      <td>0.941176</td>\n      <td>0.879518</td>\n      <td>1.000000</td>\n      <td>0.896552</td>\n      <td>0.760000</td>\n      <td>0.952381</td>\n      <td>0.703704</td>\n      <td>0.806452</td>\n      <td>0.862485</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.261900</td>\n      <td>0.520440</td>\n      <td>0.882353</td>\n      <td>0.891566</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.760000</td>\n      <td>0.809524</td>\n      <td>0.703704</td>\n      <td>0.935484</td>\n      <td>0.861379</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.261900</td>\n      <td>0.503212</td>\n      <td>0.823529</td>\n      <td>0.831325</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.820000</td>\n      <td>0.857143</td>\n      <td>0.703704</td>\n      <td>0.903226</td>\n      <td>0.855458</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.261900</td>\n      <td>0.578018</td>\n      <td>0.882353</td>\n      <td>0.903614</td>\n      <td>1.000000</td>\n      <td>0.896552</td>\n      <td>0.760000</td>\n      <td>0.904762</td>\n      <td>0.555556</td>\n      <td>0.870968</td>\n      <td>0.850417</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.261900</td>\n      <td>0.553181</td>\n      <td>0.882353</td>\n      <td>0.891566</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.780000</td>\n      <td>0.857143</td>\n      <td>0.629630</td>\n      <td>0.935484</td>\n      <td>0.857888</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.261900</td>\n      <td>0.531417</td>\n      <td>0.941176</td>\n      <td>0.879518</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.800000</td>\n      <td>0.857143</td>\n      <td>0.740741</td>\n      <td>0.870968</td>\n      <td>0.872194</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.120200</td>\n      <td>0.552348</td>\n      <td>0.882353</td>\n      <td>0.855422</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.780000</td>\n      <td>0.761905</td>\n      <td>0.703704</td>\n      <td>0.903226</td>\n      <td>0.843689</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.120200</td>\n      <td>0.571141</td>\n      <td>0.882353</td>\n      <td>0.903614</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.740000</td>\n      <td>0.904762</td>\n      <td>0.703704</td>\n      <td>0.903226</td>\n      <td>0.868436</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.120200</td>\n      <td>0.546465</td>\n      <td>0.882353</td>\n      <td>0.867470</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.780000</td>\n      <td>0.904762</td>\n      <td>0.666667</td>\n      <td>0.903226</td>\n      <td>0.859590</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.120200</td>\n      <td>0.575398</td>\n      <td>0.882353</td>\n      <td>0.915663</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.780000</td>\n      <td>0.857143</td>\n      <td>0.666667</td>\n      <td>0.870968</td>\n      <td>0.863185</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.120200</td>\n      <td>0.547361</td>\n      <td>0.882353</td>\n      <td>0.879518</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.780000</td>\n      <td>0.857143</td>\n      <td>0.703704</td>\n      <td>0.870968</td>\n      <td>0.856416</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.120200</td>\n      <td>0.551704</td>\n      <td>0.882353</td>\n      <td>0.891566</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.800000</td>\n      <td>0.857143</td>\n      <td>0.777778</td>\n      <td>0.903226</td>\n      <td>0.877981</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.120200</td>\n      <td>0.566376</td>\n      <td>0.882353</td>\n      <td>0.867470</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.800000</td>\n      <td>0.809524</td>\n      <td>0.666667</td>\n      <td>0.903226</td>\n      <td>0.853413</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.120200</td>\n      <td>0.571040</td>\n      <td>0.882353</td>\n      <td>0.903614</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.760000</td>\n      <td>0.809524</td>\n      <td>0.703704</td>\n      <td>0.903226</td>\n      <td>0.861759</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.120200</td>\n      <td>0.552486</td>\n      <td>0.882353</td>\n      <td>0.903614</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.780000</td>\n      <td>0.857143</td>\n      <td>0.740741</td>\n      <td>0.903226</td>\n      <td>0.870613</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.120200</td>\n      <td>0.568752</td>\n      <td>0.882353</td>\n      <td>0.903614</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.800000</td>\n      <td>0.761905</td>\n      <td>0.740741</td>\n      <td>0.870968</td>\n      <td>0.860489</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.120200</td>\n      <td>0.560519</td>\n      <td>0.882353</td>\n      <td>0.891566</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.800000</td>\n      <td>0.857143</td>\n      <td>0.703704</td>\n      <td>0.935484</td>\n      <td>0.868501</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.120200</td>\n      <td>0.557670</td>\n      <td>0.882353</td>\n      <td>0.891566</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.800000</td>\n      <td>0.857143</td>\n      <td>0.703704</td>\n      <td>0.870968</td>\n      <td>0.863177</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.063000</td>\n      <td>0.565865</td>\n      <td>0.882353</td>\n      <td>0.903614</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.800000</td>\n      <td>0.857143</td>\n      <td>0.703704</td>\n      <td>0.870968</td>\n      <td>0.866107</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2000, training_loss=0.34408121299743655, metrics={'train_runtime': 2196.1718, 'train_samples_per_second': 58.124, 'train_steps_per_second': 0.911, 'total_flos': 1876173062134248.0, 'train_loss': 0.34408121299743655, 'epoch': 50.0})"},"metadata":{}}]},{"cell_type":"code","source":"# Trying to maximize F1 score with optimizer an scheduler adjusting. \n# It's a good idead to find the best parameters, but i will stop here. The best result is F1 score = 0.877,\n# But I think that it may be upgraded further...\n\nfrom torch.optim import AdamW\nfrom transformers import get_linear_schedule_with_warmup\n\n# num_training_steps: epochs * batch_size\n# num_warmup_steps - number of steps for warmupping, we should set it to the number of steps where plato begin\nopt = AdamW(lastmodel.parameters(), lr=5e-05, eps=1e-08)\nscheduler = get_linear_schedule_with_warmup(opt, num_training_steps=int(len(trainDataset) // 64 * 50), num_warmup_steps=int(10 * len(trainDataset) // 64))\n\ntrainer2 = CustomTrainer(\n    model=lastmodel,\n    args=training_arguments,\n    data_collator=collator_fn,\n    train_dataset=trainDataset,\n    eval_dataset=valDataset,\n    compute_metrics=compute_metrics,\n    dataset_text_field='text',\n    optimizers=(opt, scheduler)\n)\n\ntrainer2.train()","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:58:04.083486Z","iopub.execute_input":"2024-03-20T07:58:04.083868Z","iopub.status.idle":"2024-03-20T08:36:44.831990Z","shell.execute_reply.started":"2024-03-20T07:58:04.083834Z","shell.execute_reply":"2024-03-20T08:36:44.830778Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:194: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 512\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1600' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1600/1600 38:37, Epoch 50/50]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Class 0 Recall</th>\n      <th>Class 1 Recall</th>\n      <th>Class 2 Recall</th>\n      <th>Class 3 Recall</th>\n      <th>Class 4 Recall</th>\n      <th>Class 5 Recall</th>\n      <th>Class 6 Recall</th>\n      <th>Class 7 Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>2.078713</td>\n      <td>0.705882</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.222222</td>\n      <td>0.000000</td>\n      <td>0.030896</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>2.065280</td>\n      <td>0.000000</td>\n      <td>0.349398</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.060000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.093491</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>2.038636</td>\n      <td>0.647059</td>\n      <td>0.000000</td>\n      <td>0.807692</td>\n      <td>0.103448</td>\n      <td>0.320000</td>\n      <td>0.619048</td>\n      <td>0.000000</td>\n      <td>0.064516</td>\n      <td>0.199849</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>1.930813</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.310345</td>\n      <td>0.460000</td>\n      <td>0.047619</td>\n      <td>0.703704</td>\n      <td>0.774194</td>\n      <td>0.212963</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>1.743632</td>\n      <td>0.000000</td>\n      <td>0.289157</td>\n      <td>0.884615</td>\n      <td>0.344828</td>\n      <td>0.380000</td>\n      <td>0.000000</td>\n      <td>0.111111</td>\n      <td>0.612903</td>\n      <td>0.278875</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>No log</td>\n      <td>1.593543</td>\n      <td>0.294118</td>\n      <td>0.072289</td>\n      <td>0.692308</td>\n      <td>0.827586</td>\n      <td>0.400000</td>\n      <td>0.238095</td>\n      <td>0.074074</td>\n      <td>0.774194</td>\n      <td>0.343342</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>No log</td>\n      <td>1.425735</td>\n      <td>0.705882</td>\n      <td>0.012048</td>\n      <td>0.846154</td>\n      <td>0.758621</td>\n      <td>0.760000</td>\n      <td>0.095238</td>\n      <td>0.037037</td>\n      <td>0.709677</td>\n      <td>0.411680</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>No log</td>\n      <td>1.225176</td>\n      <td>0.764706</td>\n      <td>0.385542</td>\n      <td>0.961538</td>\n      <td>0.793103</td>\n      <td>0.560000</td>\n      <td>0.428571</td>\n      <td>0.259259</td>\n      <td>0.903226</td>\n      <td>0.578902</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>No log</td>\n      <td>1.052057</td>\n      <td>0.882353</td>\n      <td>0.216867</td>\n      <td>0.961538</td>\n      <td>0.724138</td>\n      <td>0.840000</td>\n      <td>0.619048</td>\n      <td>0.370370</td>\n      <td>0.967742</td>\n      <td>0.626225</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>No log</td>\n      <td>0.910509</td>\n      <td>0.882353</td>\n      <td>0.638554</td>\n      <td>1.000000</td>\n      <td>0.724138</td>\n      <td>0.680000</td>\n      <td>0.714286</td>\n      <td>0.333333</td>\n      <td>0.935484</td>\n      <td>0.712706</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>No log</td>\n      <td>0.871056</td>\n      <td>0.941176</td>\n      <td>0.662651</td>\n      <td>1.000000</td>\n      <td>0.655172</td>\n      <td>0.620000</td>\n      <td>0.761905</td>\n      <td>0.333333</td>\n      <td>0.935484</td>\n      <td>0.695820</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>No log</td>\n      <td>0.772020</td>\n      <td>0.823529</td>\n      <td>0.734940</td>\n      <td>1.000000</td>\n      <td>0.827586</td>\n      <td>0.700000</td>\n      <td>0.666667</td>\n      <td>0.629630</td>\n      <td>0.903226</td>\n      <td>0.782828</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>No log</td>\n      <td>0.724967</td>\n      <td>0.882353</td>\n      <td>0.638554</td>\n      <td>1.000000</td>\n      <td>0.724138</td>\n      <td>0.680000</td>\n      <td>0.857143</td>\n      <td>0.370370</td>\n      <td>0.967742</td>\n      <td>0.726074</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>No log</td>\n      <td>0.663835</td>\n      <td>0.882353</td>\n      <td>0.626506</td>\n      <td>1.000000</td>\n      <td>0.724138</td>\n      <td>0.840000</td>\n      <td>0.714286</td>\n      <td>0.555556</td>\n      <td>0.935484</td>\n      <td>0.761919</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>No log</td>\n      <td>0.773984</td>\n      <td>0.823529</td>\n      <td>0.144578</td>\n      <td>1.000000</td>\n      <td>0.758621</td>\n      <td>0.640000</td>\n      <td>0.714286</td>\n      <td>0.777778</td>\n      <td>0.935484</td>\n      <td>0.670583</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>1.314800</td>\n      <td>0.677722</td>\n      <td>0.882353</td>\n      <td>0.542169</td>\n      <td>1.000000</td>\n      <td>0.793103</td>\n      <td>0.680000</td>\n      <td>0.666667</td>\n      <td>0.629630</td>\n      <td>0.935484</td>\n      <td>0.725978</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>1.314800</td>\n      <td>0.589061</td>\n      <td>0.882353</td>\n      <td>0.759036</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.760000</td>\n      <td>0.666667</td>\n      <td>0.592593</td>\n      <td>0.870968</td>\n      <td>0.783663</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>1.314800</td>\n      <td>0.565328</td>\n      <td>0.882353</td>\n      <td>0.710843</td>\n      <td>1.000000</td>\n      <td>0.827586</td>\n      <td>0.740000</td>\n      <td>0.809524</td>\n      <td>0.740741</td>\n      <td>0.903226</td>\n      <td>0.800112</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>1.314800</td>\n      <td>0.533962</td>\n      <td>0.882353</td>\n      <td>0.698795</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.740000</td>\n      <td>0.761905</td>\n      <td>0.777778</td>\n      <td>0.870968</td>\n      <td>0.804402</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.314800</td>\n      <td>0.558331</td>\n      <td>0.823529</td>\n      <td>0.698795</td>\n      <td>1.000000</td>\n      <td>0.827586</td>\n      <td>0.800000</td>\n      <td>0.666667</td>\n      <td>0.777778</td>\n      <td>0.935484</td>\n      <td>0.808102</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>1.314800</td>\n      <td>0.546220</td>\n      <td>0.882353</td>\n      <td>0.614458</td>\n      <td>1.000000</td>\n      <td>0.827586</td>\n      <td>0.800000</td>\n      <td>0.761905</td>\n      <td>0.703704</td>\n      <td>0.935484</td>\n      <td>0.783426</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>1.314800</td>\n      <td>0.546419</td>\n      <td>0.882353</td>\n      <td>0.602410</td>\n      <td>1.000000</td>\n      <td>0.896552</td>\n      <td>0.740000</td>\n      <td>0.809524</td>\n      <td>0.851852</td>\n      <td>0.903226</td>\n      <td>0.800906</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>1.314800</td>\n      <td>0.535148</td>\n      <td>0.882353</td>\n      <td>0.746988</td>\n      <td>0.961538</td>\n      <td>0.896552</td>\n      <td>0.800000</td>\n      <td>0.904762</td>\n      <td>0.629630</td>\n      <td>0.870968</td>\n      <td>0.823565</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>1.314800</td>\n      <td>0.573639</td>\n      <td>0.882353</td>\n      <td>0.602410</td>\n      <td>1.000000</td>\n      <td>0.896552</td>\n      <td>0.780000</td>\n      <td>0.809524</td>\n      <td>0.703704</td>\n      <td>0.903226</td>\n      <td>0.790403</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>1.314800</td>\n      <td>0.522746</td>\n      <td>0.882353</td>\n      <td>0.710843</td>\n      <td>0.961538</td>\n      <td>0.862069</td>\n      <td>0.780000</td>\n      <td>0.809524</td>\n      <td>0.740741</td>\n      <td>0.870968</td>\n      <td>0.809390</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>1.314800</td>\n      <td>0.584850</td>\n      <td>0.882353</td>\n      <td>0.662651</td>\n      <td>0.961538</td>\n      <td>0.896552</td>\n      <td>0.760000</td>\n      <td>0.809524</td>\n      <td>0.740741</td>\n      <td>0.870968</td>\n      <td>0.795810</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>1.314800</td>\n      <td>0.555680</td>\n      <td>0.823529</td>\n      <td>0.867470</td>\n      <td>0.923077</td>\n      <td>0.896552</td>\n      <td>0.760000</td>\n      <td>0.714286</td>\n      <td>0.666667</td>\n      <td>0.870968</td>\n      <td>0.825495</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>1.314800</td>\n      <td>0.568869</td>\n      <td>0.882353</td>\n      <td>0.759036</td>\n      <td>0.961538</td>\n      <td>0.896552</td>\n      <td>0.800000</td>\n      <td>0.761905</td>\n      <td>0.666667</td>\n      <td>0.870968</td>\n      <td>0.814286</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>1.314800</td>\n      <td>0.536226</td>\n      <td>0.823529</td>\n      <td>0.795181</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.740000</td>\n      <td>0.809524</td>\n      <td>0.740741</td>\n      <td>0.870968</td>\n      <td>0.824005</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.314800</td>\n      <td>0.514422</td>\n      <td>0.882353</td>\n      <td>0.795181</td>\n      <td>1.000000</td>\n      <td>0.896552</td>\n      <td>0.760000</td>\n      <td>0.857143</td>\n      <td>0.740741</td>\n      <td>0.870968</td>\n      <td>0.837872</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>1.314800</td>\n      <td>0.520455</td>\n      <td>0.882353</td>\n      <td>0.795181</td>\n      <td>0.961538</td>\n      <td>0.896552</td>\n      <td>0.760000</td>\n      <td>0.857143</td>\n      <td>0.777778</td>\n      <td>0.870968</td>\n      <td>0.839670</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.291600</td>\n      <td>0.612682</td>\n      <td>0.882353</td>\n      <td>0.795181</td>\n      <td>0.961538</td>\n      <td>0.896552</td>\n      <td>0.760000</td>\n      <td>0.761905</td>\n      <td>0.518519</td>\n      <td>0.870968</td>\n      <td>0.791866</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.291600</td>\n      <td>0.539684</td>\n      <td>0.882353</td>\n      <td>0.795181</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.820000</td>\n      <td>0.857143</td>\n      <td>0.666667</td>\n      <td>0.903226</td>\n      <td>0.848429</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.291600</td>\n      <td>0.598261</td>\n      <td>0.823529</td>\n      <td>0.891566</td>\n      <td>0.961538</td>\n      <td>0.862069</td>\n      <td>0.820000</td>\n      <td>0.714286</td>\n      <td>0.666667</td>\n      <td>0.806452</td>\n      <td>0.839123</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.291600</td>\n      <td>0.552956</td>\n      <td>0.882353</td>\n      <td>0.807229</td>\n      <td>1.000000</td>\n      <td>0.896552</td>\n      <td>0.780000</td>\n      <td>0.857143</td>\n      <td>0.666667</td>\n      <td>0.870968</td>\n      <td>0.841549</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.291600</td>\n      <td>0.530114</td>\n      <td>0.882353</td>\n      <td>0.795181</td>\n      <td>0.961538</td>\n      <td>0.862069</td>\n      <td>0.760000</td>\n      <td>0.904762</td>\n      <td>0.666667</td>\n      <td>0.903226</td>\n      <td>0.836373</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.291600</td>\n      <td>0.530360</td>\n      <td>0.882353</td>\n      <td>0.807229</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.800000</td>\n      <td>0.809524</td>\n      <td>0.703704</td>\n      <td>0.870968</td>\n      <td>0.843066</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.291600</td>\n      <td>0.623764</td>\n      <td>0.882353</td>\n      <td>0.819277</td>\n      <td>0.923077</td>\n      <td>0.896552</td>\n      <td>0.800000</td>\n      <td>0.761905</td>\n      <td>0.592593</td>\n      <td>0.870968</td>\n      <td>0.822377</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.291600</td>\n      <td>0.598665</td>\n      <td>0.882353</td>\n      <td>0.831325</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.760000</td>\n      <td>0.857143</td>\n      <td>0.666667</td>\n      <td>0.870968</td>\n      <td>0.837248</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.291600</td>\n      <td>0.543920</td>\n      <td>0.882353</td>\n      <td>0.855422</td>\n      <td>0.961538</td>\n      <td>0.862069</td>\n      <td>0.740000</td>\n      <td>0.761905</td>\n      <td>0.703704</td>\n      <td>0.903226</td>\n      <td>0.839133</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.291600</td>\n      <td>0.568576</td>\n      <td>0.882353</td>\n      <td>0.843373</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.740000</td>\n      <td>0.809524</td>\n      <td>0.703704</td>\n      <td>0.870968</td>\n      <td>0.834397</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.291600</td>\n      <td>0.620219</td>\n      <td>0.882353</td>\n      <td>0.819277</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.800000</td>\n      <td>0.809524</td>\n      <td>0.592593</td>\n      <td>0.903226</td>\n      <td>0.830736</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.291600</td>\n      <td>0.586619</td>\n      <td>0.882353</td>\n      <td>0.819277</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.780000</td>\n      <td>0.761905</td>\n      <td>0.703704</td>\n      <td>0.903226</td>\n      <td>0.840582</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.291600</td>\n      <td>0.596455</td>\n      <td>0.882353</td>\n      <td>0.867470</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.780000</td>\n      <td>0.809524</td>\n      <td>0.666667</td>\n      <td>0.870968</td>\n      <td>0.848912</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.291600</td>\n      <td>0.590882</td>\n      <td>0.823529</td>\n      <td>0.891566</td>\n      <td>0.961538</td>\n      <td>0.862069</td>\n      <td>0.740000</td>\n      <td>0.761905</td>\n      <td>0.740741</td>\n      <td>0.870968</td>\n      <td>0.847078</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.291600</td>\n      <td>0.605417</td>\n      <td>0.882353</td>\n      <td>0.819277</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.800000</td>\n      <td>0.857143</td>\n      <td>0.592593</td>\n      <td>0.903226</td>\n      <td>0.838836</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.104800</td>\n      <td>0.582579</td>\n      <td>0.823529</td>\n      <td>0.867470</td>\n      <td>0.961538</td>\n      <td>0.862069</td>\n      <td>0.800000</td>\n      <td>0.857143</td>\n      <td>0.703704</td>\n      <td>0.903226</td>\n      <td>0.860037</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.104800</td>\n      <td>0.655619</td>\n      <td>0.882353</td>\n      <td>0.879518</td>\n      <td>1.000000</td>\n      <td>0.862069</td>\n      <td>0.760000</td>\n      <td>0.761905</td>\n      <td>0.629630</td>\n      <td>0.838710</td>\n      <td>0.837795</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.104800</td>\n      <td>0.644455</td>\n      <td>0.882353</td>\n      <td>0.891566</td>\n      <td>1.000000</td>\n      <td>0.827586</td>\n      <td>0.740000</td>\n      <td>0.761905</td>\n      <td>0.666667</td>\n      <td>0.903226</td>\n      <td>0.844953</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.104800</td>\n      <td>0.653516</td>\n      <td>0.882353</td>\n      <td>0.879518</td>\n      <td>0.961538</td>\n      <td>0.862069</td>\n      <td>0.780000</td>\n      <td>0.809524</td>\n      <td>0.592593</td>\n      <td>0.870968</td>\n      <td>0.837867</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1600, training_loss=0.5382260099053383, metrics={'train_runtime': 2319.5272, 'train_samples_per_second': 55.033, 'train_steps_per_second': 0.69, 'total_flos': 3089378525547432.0, 'train_loss': 0.5382260099053383, 'epoch': 50.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"* Testing","metadata":{}},{"cell_type":"code","source":"def makePrediction(text, model, ind2label):\n    model.eval()\n    inputs = collator_fn([trainDataset.prepare_example(text)])\n    outputs = model(**inputs, ).logits\n    label = torch.argmax(outputs).item()\n    decodedLabel = ind2label[label]\n    return decodedLabel","metadata":{"execution":{"iopub.status.busy":"2024-03-20T11:07:56.724582Z","iopub.execute_input":"2024-03-20T11:07:56.724939Z","iopub.status.idle":"2024-03-20T11:07:56.731830Z","shell.execute_reply.started":"2024-03-20T11:07:56.724912Z","shell.execute_reply":"2024-03-20T11:07:56.730841Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"prompts =  [\n    'я не знаю как еще описать этот ужас, продавцы наглые, грубят. Я попросила дать мне пакет, а тот в свою очередь сказал, что у него нет настроения и он не готов меня обслуживать, ужас!',\n    'этот какой то кошмар. Магазин галерея Краснодар худший в мире! Заказала себе стиральную машину и стояла в очереди 2 часа!!! уже сил нет, а я еще и с ребенком была',\n    'позвонила в колл центр и в первые за всю свою жизнь встретила такое хамское отношение, как бы я не старалась объяснить свою проблему оператору, она так и не смогла ничем мне помочь, так еще и послала, беда'\n]\n\nfrom IPython.display import HTML, display\ntable_template = \"\"\"<table style=\"border:1px solid black\" >\n  <tr>\n    <th style=\"text-align: center; border:1px solid black\">FEEDBACK</th>\n    <th style=\"text-align: center; border:1px solid black\">CLASS</th>\n  </tr>\n{}\n</table>\"\"\"\n\nrow_template = '''  <tr>\n    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">{}</pre></td>\n    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">{}</pre></td>\n  </tr>'''\n\nrows = []\nfor prompt in prompts:\n    prediction = makePrediction(prompt, lastmodel, ind2label)\n    rows.append(row_template.format(prompt, prediction))\ndisplay(HTML(table_template.format('\\n'.join(rows))))","metadata":{"execution":{"iopub.status.busy":"2024-03-20T11:24:24.659299Z","iopub.execute_input":"2024-03-20T11:24:24.660079Z","iopub.status.idle":"2024-03-20T11:24:24.745030Z","shell.execute_reply.started":"2024-03-20T11:24:24.660047Z","shell.execute_reply":"2024-03-20T11:24:24.743970Z"},"trusted":true},"execution_count":101,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table style=\"border:1px solid black\" >\n  <tr>\n    <th style=\"text-align: center; border:1px solid black\">FEEDBACK</th>\n    <th style=\"text-align: center; border:1px solid black\">CLASS</th>\n  </tr>\n  <tr>\n    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">я не знаю как еще описать этот ужас, продавцы наглые, грубят. Я попросила дать мне пакет, а тот в свою очередь сказал, что у него нет настроения и он не готов меня обслуживать, ужас!</pre></td>\n    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">Вежливость сотрудников магазина</pre></td>\n  </tr>\n  <tr>\n    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">этот какой то кошмар. Магазин галерея Краснодар худший в мире! Заказала себе стиральную машину и стояла в очереди 2 часа!!! уже сил нет, а я еще и с ребенком была</pre></td>\n    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">Время ожидания у кассы</pre></td>\n  </tr>\n  <tr>\n    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">позвонила в колл центр и в первые за всю свою жизнь встретила такое хамское отношение, как бы я не старалась объяснить свою проблему оператору, она так и не смогла ничем мне помочь, так еще и послала, беда</pre></td>\n    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">Консультация КЦ</pre></td>\n  </tr>\n</table>"},"metadata":{}}]},{"cell_type":"markdown","source":"# So, this is the end of my work, i think that results are pretty good, with custom feedback model give good results!","metadata":{}}]}